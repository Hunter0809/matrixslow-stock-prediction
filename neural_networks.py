#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¿®å¤ç‰ˆç¥ç»ç½‘ç»œæ¨¡å—ï¼šè§£å†³LSTMæŸå¤±ä¸ä¸‹é™é—®é¢˜
ä¸»è¦ä¿®å¤ï¼š
1. LSTMé—¨è®¡ç®—ç»“æ„
2. å‚æ•°åˆå§‹åŒ–ç­–ç•¥
3. å‰å‘ä¼ æ’­é€»è¾‘
4. æ¢¯åº¦è®¡ç®—ä¼˜åŒ–
"""

import numpy as np
from core_framework import Parameter, Variable
from operators import Add, MatMul, Multiply, Tanh, Sigmoid, ReLU


class Linear:
    """ä¼˜åŒ–åçš„å…¨è¿æ¥å±‚"""

    def __init__(self, in_features, out_features, name=''):
        self.in_features = in_features
        self.out_features = out_features
        self.name = name

        # ä½¿ç”¨Heåˆå§‹åŒ–ï¼Œæ›´é€‚åˆæ·±åº¦ç½‘ç»œ
        scale = np.sqrt(2.0 / in_features)

        w_data = np.random.normal(0, scale, (in_features, out_features))
        b_data = np.zeros((1, out_features))

        self.W = Parameter(w_data, f'{name}_W')
        self.b = Parameter(b_data, f'{name}_b')

    def __call__(self, x):
        if x is None or x.value is None:
            raise ValueError(f"Input to Linear layer {self.name} has no value")

        # åˆ›å»ºè®¡ç®—èŠ‚ç‚¹å¹¶ç«‹å³æ‰§è¡Œå‰å‘ä¼ æ’­
        matmul_node = MatMul(x, self.W)
        matmul_node.forward()

        add_node = Add(matmul_node, self.b)
        add_node.forward()

        return add_node

    def get_parameters(self):
        return [self.W, self.b]


class SimplifiedLSTMCell:
    """ç®€åŒ–ä½†æ­£ç¡®çš„LSTMå•å…ƒå®ç°"""

    def __init__(self, input_size, hidden_size, name=''):
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.name = name

        # æ”¹è¿›çš„åˆå§‹åŒ–ç­–ç•¥
        scale = 1.0 / np.sqrt(hidden_size)

        # è¾“å…¥é—¨æƒé‡
        self.W_ii = Parameter(np.random.uniform(-scale, scale, (input_size, hidden_size)), f'{name}_W_ii')
        self.W_hi = Parameter(np.random.uniform(-scale, scale, (hidden_size, hidden_size)), f'{name}_W_hi')
        self.b_i = Parameter(np.zeros((1, hidden_size)), f'{name}_b_i')

        # é—å¿˜é—¨æƒé‡ - åç½®åˆå§‹åŒ–ä¸º1
        self.W_if = Parameter(np.random.uniform(-scale, scale, (input_size, hidden_size)), f'{name}_W_if')
        self.W_hf = Parameter(np.random.uniform(-scale, scale, (hidden_size, hidden_size)), f'{name}_W_hf')
        self.b_f = Parameter(np.ones((1, hidden_size)), f'{name}_b_f')  # é—å¿˜é—¨åç½®ä¸º1

        # å€™é€‰çŠ¶æ€æƒé‡
        self.W_ig = Parameter(np.random.uniform(-scale, scale, (input_size, hidden_size)), f'{name}_W_ig')
        self.W_hg = Parameter(np.random.uniform(-scale, scale, (hidden_size, hidden_size)), f'{name}_W_hg')
        self.b_g = Parameter(np.zeros((1, hidden_size)), f'{name}_b_g')

        # è¾“å‡ºé—¨æƒé‡
        self.W_io = Parameter(np.random.uniform(-scale, scale, (input_size, hidden_size)), f'{name}_W_io')
        self.W_ho = Parameter(np.random.uniform(-scale, scale, (hidden_size, hidden_size)), f'{name}_W_ho')
        self.b_o = Parameter(np.zeros((1, hidden_size)), f'{name}_b_o')

    def __call__(self, x, h_prev, c_prev):
        """ç®€åŒ–ä½†æ­£ç¡®çš„LSTMå‰å‘ä¼ æ’­"""
        # è¾“å…¥é—¨ï¼ši = sigmoid(W_ii * x + W_hi * h + b_i)
        i_x = MatMul(x, self.W_ii)
        i_x.forward()
        i_h = MatMul(h_prev, self.W_hi)
        i_h.forward()
        i_linear = Add(Add(i_x, i_h), self.b_i)
        i_linear.forward()
        i_gate = Sigmoid(i_linear)
        i_gate.forward()

        # é—å¿˜é—¨ï¼šf = sigmoid(W_if * x + W_hf * h + b_f)
        f_x = MatMul(x, self.W_if)
        f_x.forward()
        f_h = MatMul(h_prev, self.W_hf)
        f_h.forward()
        f_linear = Add(Add(f_x, f_h), self.b_f)
        f_linear.forward()
        f_gate = Sigmoid(f_linear)
        f_gate.forward()

        # å€™é€‰çŠ¶æ€ï¼šg = tanh(W_ig * x + W_hg * h + b_g)
        g_x = MatMul(x, self.W_ig)
        g_x.forward()
        g_h = MatMul(h_prev, self.W_hg)
        g_h.forward()
        g_linear = Add(Add(g_x, g_h), self.b_g)
        g_linear.forward()
        g_gate = Tanh(g_linear)
        g_gate.forward()

        # è¾“å‡ºé—¨ï¼šo = sigmoid(W_io * x + W_ho * h + b_o)
        o_x = MatMul(x, self.W_io)
        o_x.forward()
        o_h = MatMul(h_prev, self.W_ho)
        o_h.forward()
        o_linear = Add(Add(o_x, o_h), self.b_o)
        o_linear.forward()
        o_gate = Sigmoid(o_linear)
        o_gate.forward()

        # æ›´æ–°ç»†èƒçŠ¶æ€ï¼šc_new = f * c_prev + i * g
        fc = Multiply(f_gate, c_prev)
        fc.forward()
        ig = Multiply(i_gate, g_gate)
        ig.forward()
        c_new = Add(fc, ig)
        c_new.forward()

        # æ›´æ–°éšè—çŠ¶æ€ï¼šh_new = o * tanh(c_new)
        c_tanh = Tanh(c_new)
        c_tanh.forward()
        h_new = Multiply(o_gate, c_tanh)
        h_new.forward()

        return h_new, c_new

    def get_parameters(self):
        return [
            self.W_ii, self.W_hi, self.b_i,
            self.W_if, self.W_hf, self.b_f,
            self.W_ig, self.W_hg, self.b_g,
            self.W_io, self.W_ho, self.b_o
        ]


class LSTM:
    """ä¿®å¤åçš„LSTMå±‚"""

    def __init__(self, input_size, hidden_size, num_layers=1, name=''):
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.name = name

        # åˆ›å»ºç®€åŒ–çš„LSTMå•å…ƒ
        self.cells = []
        for i in range(num_layers):
            if i == 0:
                cell = SimplifiedLSTMCell(input_size, hidden_size, f'{name}_layer{i}')
            else:
                cell = SimplifiedLSTMCell(hidden_size, hidden_size, f'{name}_layer{i}')
            self.cells.append(cell)

    def __call__(self, x_sequence, initial_states=None):
        """ä¿®å¤åçš„LSTMå‰å‘ä¼ æ’­"""
        if not x_sequence:
            raise ValueError("x_sequence cannot be empty")

        if x_sequence[0].value is None:
            raise ValueError("First element of x_sequence has no value")

        batch_size = x_sequence[0].value.shape[0]
        seq_len = len(x_sequence)

        # åˆå§‹åŒ–çŠ¶æ€
        if initial_states is None:
            h_states = []
            c_states = []
            for i in range(self.num_layers):
                h_init = Variable((batch_size, self.hidden_size))
                h_init.set_value(np.zeros((batch_size, self.hidden_size)))

                c_init = Variable((batch_size, self.hidden_size))
                c_init.set_value(np.zeros((batch_size, self.hidden_size)))

                h_states.append(h_init)
                c_states.append(c_init)
        else:
            h_states, c_states = initial_states

        outputs = []

        # é€æ—¶é—´æ­¥å¤„ç†
        for t in range(seq_len):
            x = x_sequence[t]

            if x.value is None:
                raise ValueError(f"x_sequence[{t}] has no value")

            # é€šè¿‡å„å±‚
            current_input = x
            for layer in range(self.num_layers):
                h_new, c_new = self.cells[layer](current_input, h_states[layer], c_states[layer])
                h_states[layer] = h_new
                c_states[layer] = c_new
                current_input = h_new

            outputs.append(h_states[-1])

        return outputs, (h_states, c_states)

    def get_parameters(self):
        """è·å–æ‰€æœ‰å‚æ•°"""
        params = []
        for cell in self.cells:
            params.extend(cell.get_parameters())
        return params


class RNNCell:
    """ç®€åŒ–çš„RNNå•å…ƒ"""

    def __init__(self, input_size, hidden_size, name=''):
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.name = name

        scale = 1.0 / np.sqrt(hidden_size)

        self.W_ih = Parameter(np.random.uniform(-scale, scale, (input_size, hidden_size)), f'{name}_W_ih')
        self.W_hh = Parameter(np.random.uniform(-scale, scale, (hidden_size, hidden_size)), f'{name}_W_hh')
        self.b_h = Parameter(np.zeros((1, hidden_size)), f'{name}_b_h')

    def __call__(self, x, h_prev):
        """RNNå‰å‘ä¼ æ’­"""
        ih_linear = MatMul(x, self.W_ih)
        ih_linear.forward()

        hh_linear = MatMul(h_prev, self.W_hh)
        hh_linear.forward()

        linear = Add(Add(ih_linear, hh_linear), self.b_h)
        linear.forward()

        h_new = Tanh(linear)
        h_new.forward()

        return h_new

    def get_parameters(self):
        return [self.W_ih, self.W_hh, self.b_h]


class RNN:
    """ä¿®å¤åçš„RNNå±‚"""

    def __init__(self, input_size, hidden_size, num_layers=1, name=''):
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.name = name

        self.cells = []
        for i in range(num_layers):
            if i == 0:
                cell = RNNCell(input_size, hidden_size, f'{name}_layer{i}')
            else:
                cell = RNNCell(hidden_size, hidden_size, f'{name}_layer{i}')
            self.cells.append(cell)

    def __call__(self, x_sequence, initial_states=None):
        """RNNå‰å‘ä¼ æ’­"""
        if not x_sequence:
            raise ValueError("x_sequence cannot be empty")

        batch_size = x_sequence[0].value.shape[0]
        seq_len = len(x_sequence)

        # åˆå§‹åŒ–éšè—çŠ¶æ€
        if initial_states is None:
            h_states = []
            for i in range(self.num_layers):
                h_init = Variable((batch_size, self.hidden_size))
                h_init.set_value(np.zeros((batch_size, self.hidden_size)))
                h_states.append(h_init)
        else:
            h_states = initial_states

        outputs = []

        # å¤„ç†åºåˆ—
        for t in range(seq_len):
            x = x_sequence[t]

            # é€šè¿‡å„å±‚
            current_input = x
            for layer in range(self.num_layers):
                h_new = self.cells[layer](current_input, h_states[layer])
                h_states[layer] = h_new
                current_input = h_new

            outputs.append(h_states[-1])

        return outputs, h_states

    def get_parameters(self):
        params = []
        for cell in self.cells:
            params.extend(cell.get_parameters())
        return params


class MLP:
    """å¤šå±‚æ„ŸçŸ¥æœº"""

    def __init__(self, input_size, hidden_sizes, output_size, activation='relu', name=''):
        self.name = name
        self.layers = []
        self.activation = activation

        # æ„å»ºç½‘ç»œå±‚
        sizes = [input_size] + hidden_sizes + [output_size]
        for i in range(len(sizes) - 1):
            layer = Linear(sizes[i], sizes[i + 1], f'{name}_layer{i}')
            self.layers.append(layer)

    def __call__(self, x):
        """å‰å‘ä¼ æ’­"""
        if x is None or x.value is None:
            raise ValueError(f"Input to MLP {self.name} has no value")

        current = x

        for i, layer in enumerate(self.layers):
            # é€šè¿‡çº¿æ€§å±‚
            current = layer(current)

            # é™¤äº†æœ€åä¸€å±‚ï¼Œéƒ½åŠ æ¿€æ´»å‡½æ•°
            if i < len(self.layers) - 1:
                if self.activation == 'relu':
                    activation_node = ReLU(current)
                    activation_node.forward()
                    current = activation_node
                elif self.activation == 'tanh':
                    activation_node = Tanh(current)
                    activation_node.forward()
                    current = activation_node

        return current

    def get_parameters(self):
        params = []
        for layer in self.layers:
            params.extend(layer.get_parameters())
        return params


class Attention:
    """æ³¨æ„åŠ›æœºåˆ¶"""

    def __init__(self, hidden_size, name=''):
        self.hidden_size = hidden_size
        self.name = name

        # ä½¿ç”¨æ›´å°çš„åˆå§‹åŒ–
        scale = 0.1 / np.sqrt(hidden_size)

        self.W_a = Parameter(np.random.normal(0, scale, (hidden_size, hidden_size)), f'{name}_W_a')
        self.v_a = Parameter(np.random.normal(0, scale, (hidden_size, 1)), f'{name}_v_a')

    def __call__(self, hidden_states):
        """è®¡ç®—æ³¨æ„åŠ›"""
        if not hidden_states:
            raise ValueError("hidden_states cannot be empty")

        # æ£€æŸ¥æ‰€æœ‰éšè—çŠ¶æ€çš„æœ‰æ•ˆæ€§
        for i, h in enumerate(hidden_states):
            if h.value is None:
                raise ValueError(f"hidden_states[{i}] has no value")

        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        scores = []
        for h_t in hidden_states:
            # e_t = v_a^T * tanh(W_a * h_t)
            transformed = MatMul(h_t, self.W_a)
            transformed.forward()

            activated = Tanh(transformed)
            activated.forward()

            score = MatMul(activated, self.v_a)
            score.forward()

            scores.append(score)

        # ç®€åŒ–çš„softmaxè®¡ç®—
        max_score_val = max(score.value.max() for score in scores)

        exp_scores = []
        sum_exp_val = 0

        for score in scores:
            exp_val = np.exp(score.value - max_score_val)
            exp_score = Variable(exp_val.shape)
            exp_score.set_value(exp_val)
            exp_scores.append(exp_score)
            sum_exp_val += exp_val.sum()

        # è®¡ç®—æƒé‡
        weights = []
        for exp_score in exp_scores:
            weight_val = exp_score.value / sum_exp_val
            weight = Variable(weight_val.shape)
            weight.set_value(weight_val)
            weights.append(weight)

        # åŠ æƒæ±‚å’Œå¾—åˆ°ä¸Šä¸‹æ–‡å‘é‡
        context_val = np.zeros_like(hidden_states[0].value)
        for w, h in zip(weights, hidden_states):
            weighted_val = w.value * h.value
            context_val += weighted_val

        context = Variable(context_val.shape)
        context.set_value(context_val)

        return context, weights

    def get_parameters(self):
        return [self.W_a, self.v_a]


# æ¨¡å‹é…ç½®ç±»
class ModelConfig:
    """æ¨¡å‹é…ç½®ç±»ï¼Œç”¨äºå‚æ•°å˜åŒ–å®éªŒ"""

    def __init__(self, model_type, **kwargs):
        self.model_type = model_type
        self.config = kwargs

    def __str__(self):
        config_str = ", ".join([f"{k}={v}" for k, v in self.config.items()])
        return f"{self.model_type}({config_str})"


def get_model_configurations():
    """è·å–ä¸åŒçš„æ¨¡å‹é…ç½®ç”¨äºå¯¹æ¯”å®éªŒ"""
    configs = {
        # LSTMå‚æ•°å˜åŒ–å®éªŒ
        'LSTM_Small': ModelConfig('LSTM', hidden_size=16, num_layers=1),
        'LSTM_Medium': ModelConfig('LSTM', hidden_size=32, num_layers=1),
        'LSTM_Large': ModelConfig('LSTM', hidden_size=64, num_layers=1),
        'LSTM_Deep': ModelConfig('LSTM', hidden_size=32, num_layers=2),

        # RNNé…ç½®
        'RNN_Small': ModelConfig('RNN', hidden_size=32, num_layers=1),
        'RNN_Medium': ModelConfig('RNN', hidden_size=64, num_layers=1),

        # MLPé…ç½®
        'MLP_Small': ModelConfig('MLP', hidden_layers=[32, 16]),
        'MLP_Medium': ModelConfig('MLP', hidden_layers=[64, 32]),
        'MLP_Large': ModelConfig('MLP', hidden_layers=[128, 64, 32]),

        # æ³¨æ„åŠ›æœºåˆ¶é…ç½®
        'LSTM_Attention_Small': ModelConfig('LSTM_Attention', hidden_size=32, num_layers=1),
        'LSTM_Attention_Medium': ModelConfig('LSTM_Attention', hidden_size=64, num_layers=1),
    }

    return configs


def create_model_from_config(config, input_size, seq_len=None):
    """æ ¹æ®é…ç½®åˆ›å»ºæ¨¡å‹"""
    if config.model_type == 'LSTM':
        return LSTM(
            input_size=input_size,
            hidden_size=config.config['hidden_size'],
            num_layers=config.config['num_layers'],
            name=str(config)
        )
    elif config.model_type == 'RNN':
        return RNN(
            input_size=input_size,
            hidden_size=config.config['hidden_size'],
            num_layers=config.config['num_layers'],
            name=str(config)
        )
    elif config.model_type == 'MLP':
        mlp_input_size = seq_len if seq_len else input_size
        return MLP(
            input_size=mlp_input_size,
            hidden_sizes=config.config['hidden_layers'],
            output_size=1,
            activation='relu',
            name=str(config)
        )
    elif config.model_type == 'LSTM_Attention':
        lstm = LSTM(
            input_size=input_size,
            hidden_size=config.config['hidden_size'],
            num_layers=config.config['num_layers'],
            name=f"lstm_{str(config)}"
        )
        attention = Attention(
            hidden_size=config.config['hidden_size'],
            name=f"attention_{str(config)}"
        )
        return lstm, attention
    else:
        raise ValueError(f"Unknown model type: {config.model_type}")


# æµ‹è¯•ä¿®å¤åçš„å®ç°
def test_fixed_models():
    """æµ‹è¯•ä¿®å¤åçš„æ¨¡å‹å®ç°"""
    print("ğŸ§ª Testing fixed model implementations...")

    # æµ‹è¯•å‚æ•°
    batch_size = 4
    seq_len = 10
    input_size = 1
    hidden_size = 8

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    x_sequence = []
    for t in range(seq_len):
        x_t = Variable((batch_size, input_size))
        x_t.set_value(np.random.randn(batch_size, input_size) * 0.1)
        x_sequence.append(x_t)

    try:
        # æµ‹è¯•LSTM
        print("  Testing LSTM...")
        lstm = LSTM(input_size, hidden_size, num_layers=1, name='test_lstm')
        outputs, _ = lstm(x_sequence)
        print(f"  âœ… LSTM output shape: {outputs[-1].value.shape}")

        # æµ‹è¯•è¾“å‡ºå±‚
        output_layer = Linear(hidden_size, 1, 'test_output')
        pred = output_layer(outputs[-1])
        print(f"  âœ… Final prediction shape: {pred.value.shape}")

        # æµ‹è¯•RNN
        print("  Testing RNN...")
        rnn = RNN(input_size, hidden_size, num_layers=1, name='test_rnn')
        rnn_outputs, _ = rnn(x_sequence)
        print(f"  âœ… RNN output shape: {rnn_outputs[-1].value.shape}")

        # æµ‹è¯•MLP
        print("  Testing MLP...")
        X_flat = np.random.randn(batch_size, seq_len)
        x_input = Variable((batch_size, seq_len))
        x_input.set_value(X_flat)

        mlp = MLP(seq_len, [32, 16], 1, 'relu', 'test_mlp')
        mlp_pred = mlp(x_input)
        print(f"  âœ… MLP output shape: {mlp_pred.value.shape}")

        print("ğŸ‰ All model tests passed!")
        return True

    except Exception as e:
        print(f"âŒ Model test failed: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = test_fixed_models()

    if success:
        print("\nğŸ“‹ ä¸»è¦ä¿®å¤å†…å®¹:")
        print("1. âœ… ä¿®å¤LSTMé—¨è®¡ç®— - æ¯ä¸ªé—¨ç‹¬ç«‹è®¡ç®—å¹¶æ­£ç¡®å‰å‘ä¼ æ’­")
        print("2. âœ… æ”¹è¿›å‚æ•°åˆå§‹åŒ– - é—å¿˜é—¨åç½®è®¾ä¸º1ï¼Œå…¶ä»–æ›´åˆç†çš„åˆå§‹åŒ–")
        print("3. âœ… ä¿®å¤å‰å‘ä¼ æ’­ - ç¡®ä¿æ¯ä¸ªè®¡ç®—èŠ‚ç‚¹éƒ½è°ƒç”¨forward()")
        print("4. âœ… ç®€åŒ–æ³¨æ„åŠ›æœºåˆ¶ - é¿å…å¤æ‚çš„å¼ é‡æ“ä½œ")
        print("5. âœ… ä¼˜åŒ–RNNå’ŒMLPå®ç°")
        print("\nğŸš€ ç°åœ¨å¯ä»¥ç”¨è¿™ä¸ªä¿®å¤ç‰ˆæœ¬æ›¿æ¢åŸæ¥çš„neural_networks.py!")
    else:
        print("âŒ è¿˜æœ‰é—®é¢˜éœ€è¦è¿›ä¸€æ­¥ä¿®å¤")