# åŸºäºMatrixSlowæ¡†æ¶çš„è‚¡ç¥¨æ—¶åºé¢„æµ‹æ¨¡å‹ç ”ç©¶ä¸å®ç°

## ğŸ“‹ é¡¹ç›®æ¦‚è¿°

æœ¬é¡¹ç›®åŸºäºè‡ªä¸»å®ç°çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ **MatrixSlow** æ„å»ºäº†ä¸€ä¸ªå®Œæ•´çš„è‚¡ç¥¨å¼€ç›˜ä»·é¢„æµ‹ç³»ç»Ÿã€‚è¯¥é¡¹ç›®æ˜¯æ·±åœ³å¤§å­¦"åŸºäºPythonçš„æ·±åº¦å­¦ä¹ æ¡†æ¶å®ç°"çš„è¯¾ç¨‹è®¾è®¡ä½œå“ï¼Œå®Œæ•´å®ç°äº†è¯¾ç¨‹è®¾è®¡çš„æ‰€æœ‰è¦æ±‚ã€‚

### æ ¸å¿ƒç‰¹æ€§

- âœ… **å®Œå…¨åŸºäºMatrixSlow**ï¼šæ‰€æœ‰ç¥ç»ç½‘ç»œç»„ä»¶å‡ä»é›¶å¼€å§‹å®ç°ï¼Œæ— ç¬¬ä¸‰æ–¹æ¡†æ¶ä¾èµ–
- âœ… **å¤šæ¨¡å‹æ”¯æŒ**ï¼šå®ç°äº†LSTMã€RNNã€MLPä»¥åŠå¸¦æ³¨æ„åŠ›æœºåˆ¶çš„LSTM
- âœ… **å®Œæ•´æµç¨‹**ï¼šåŒ…å«æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹è®­ç»ƒã€è¯„ä¼°å’Œå¯è§†åŒ–
- âœ… **æ·±åº¦åˆ†æ**ï¼šæä¾›æ¨¡å‹å¯¹æ¯”ã€å‚æ•°è°ƒä¼˜å’Œæ€§èƒ½åˆ†æ
- âœ… **ä¸“ä¸šå¯è§†åŒ–**ï¼šç”Ÿæˆé«˜è´¨é‡å›¾è¡¨å¹¶æ”¯æŒè‡ªåŠ¨ä¿å­˜

## ğŸ¯ è¯¾ç¨‹è®¾è®¡è¦æ±‚å®Œæˆæƒ…å†µ

| è¦æ±‚ | åˆ†å€¼ | å®Œæˆæƒ…å†µ |
|------|------|----------|
| åŸºç¡€LSTMå®ç°ä¸30å¤©é¢„æµ‹ | 40åˆ† | âœ… å®Œæˆ |
| LSTMå‚æ•°å˜åŒ–å®éªŒ | 10åˆ† | âœ… å®Œæˆ |
| LSTM+æ³¨æ„åŠ›æœºåˆ¶ | 15åˆ† | âœ… å®Œæˆ |
| MLPå’ŒRNNå¯¹æ¯”åˆ†æ | 15åˆ† | âœ… å®Œæˆ |
| **æ€»è®¡** | **80åˆ†** | **å…¨éƒ¨å®Œæˆ** |

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒè¦æ±‚

```bash
Python >= 3.7
numpy >= 1.19.0
pandas >= 1.1.0
matplotlib >= 3.3.0
tqdm >= 4.62.0
```

### å®‰è£…æ­¥éª¤

```bash
# 1. å…‹éš†é¡¹ç›®
git clone <é¡¹ç›®åœ°å€>
cd matrixslow-stock-prediction

# 2. å®‰è£…ä¾èµ–
pip install numpy pandas matplotlib tqdm
```

### è¿è¡Œå®éªŒ

```bash
# è¿è¡Œå®Œæ•´å®éªŒï¼ˆåŒ…å«æ‰€æœ‰4ä¸ªå®éªŒï¼‰
python main.py
```

## ğŸ“ é¡¹ç›®ç»“æ„

```
matrixslow-stock-prediction/
â”‚
â”œâ”€â”€ core_framework.py          # MatrixSlowæ ¸å¿ƒæ¡†æ¶ï¼ˆè®¡ç®—å›¾ã€èŠ‚ç‚¹åŸºç±»ï¼‰
â”œâ”€â”€ operators.py               # æ•°å­¦ç®—å­ï¼ˆåŠ æ³•ã€çŸ©é˜µä¹˜æ³•ã€æ¿€æ´»å‡½æ•°ç­‰ï¼‰
â”œâ”€â”€ neural_networks.py         # ç¥ç»ç½‘ç»œå®ç°ï¼ˆLSTMã€RNNã€MLPã€æ³¨æ„åŠ›æœºåˆ¶ï¼‰
â”œâ”€â”€ optimizers.py              # ä¼˜åŒ–å™¨ï¼ˆSGDã€Adamï¼‰
â”œâ”€â”€ data_utils.py              # æ•°æ®å¤„ç†å·¥å…·
â”œâ”€â”€ training_utils.py          # è®­ç»ƒå·¥å…·å‡½æ•°
â”œâ”€â”€ visualization.py           # å¯è§†åŒ–å’Œåˆ†æå·¥å…·
â”œâ”€â”€ main.py                    # ä¸»ç¨‹åºå…¥å£
â”œâ”€â”€ Daily_ZX.csv              # ä¸­å…´è‚¡ç¥¨æ•°æ®
â””â”€â”€ README.md                  # æœ¬æ–‡ä»¶
```

## ğŸ”¬ MatrixSlowæ¡†æ¶åŸç†

### 1. è®¡ç®—å›¾åŸç†

MatrixSlowé‡‡ç”¨**åŠ¨æ€è®¡ç®—å›¾**æœºåˆ¶ï¼Œæ ¸å¿ƒæ¦‚å¿µåŒ…æ‹¬ï¼š

#### 1.1 è®¡ç®—å›¾æ„å»º
```python
# è®¡ç®—å›¾ç”±èŠ‚ç‚¹(Node)å’Œè¾¹ç»„æˆ
# èŠ‚ç‚¹è¡¨ç¤ºå˜é‡æˆ–æ“ä½œï¼Œè¾¹è¡¨ç¤ºæ•°æ®æµå‘
class Graph:
    def __init__(self):
        self.nodes = []      # æ‰€æœ‰èŠ‚ç‚¹åˆ—è¡¨
        self.node_dict = {}  # èŠ‚ç‚¹åç§°ç´¢å¼•
```

#### 1.2 è‡ªåŠ¨å¾®åˆ†åŸç†

åŸºäº**é“¾å¼æ³•åˆ™**å®ç°åå‘ä¼ æ’­ï¼š

å¯¹äºå¤åˆå‡½æ•° $y = f(g(x))$ï¼Œå…¶å¯¼æ•°ä¸ºï¼š
$$\frac{dy}{dx} = \frac{dy}{dg} \cdot \frac{dg}{dx}$$

åœ¨è®¡ç®—å›¾ä¸­ï¼Œæ¯ä¸ªèŠ‚ç‚¹ç»´æŠ¤**é›…å¯æ¯”çŸ©é˜µ**ï¼š
$$J = \frac{\partial \text{output}}{\partial \text{input}}$$

åå‘ä¼ æ’­æ—¶ï¼Œæ¢¯åº¦é€šè¿‡é›…å¯æ¯”çŸ©é˜µä¼ é€’ï¼š
```python
def backward(self, result):
    if self is result:
        self.jacobi = np.eye(self.dimension())
    else:
        self.jacobi = np.zeros((result.dimension(), self.dimension()))
        for child in self.children:
            if child.value is not None:
                self.jacobi += child.backward(result) * child.get_jacobi(self)
    return self.jacobi
```

### 2. æ ¸å¿ƒç®—å­æ•°å­¦åŸç†

#### 2.1 çŸ©é˜µä¹˜æ³• (MatMul)

å‰å‘ä¼ æ’­ï¼š
$$C = A \times B$$

åå‘ä¼ æ’­ï¼ˆé›…å¯æ¯”çŸ©é˜µï¼‰ï¼š
- å¯¹Açš„å¯¼æ•°ï¼š$\frac{\partial C}{\partial A} = B^T$
- å¯¹Bçš„å¯¼æ•°ï¼š$\frac{\partial C}{\partial B} = A^T$

#### 2.2 æ¿€æ´»å‡½æ•°

**Sigmoidå‡½æ•°**ï¼š
$$\sigma(x) = \frac{1}{1 + e^{-x}}$$
å¯¼æ•°ï¼š
$$\sigma'(x) = \sigma(x)(1 - \sigma(x))$$

**Tanhå‡½æ•°**ï¼š
$$\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$$
å¯¼æ•°ï¼š
$$\tanh'(x) = 1 - \tanh^2(x)$$

**ReLUå‡½æ•°**ï¼š
$$\text{ReLU}(x) = \max(0, x)$$
å¯¼æ•°ï¼š
$$\text{ReLU}'(x) = \begin{cases} 1 & \text{if } x > 0 \\ 0 & \text{if } x \leq 0 \end{cases}$$

## ğŸ§  ç¥ç»ç½‘ç»œæ¨¡å‹åŸç†

### 1. LSTMï¼ˆé•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼‰

LSTMé€šè¿‡é—¨æ§æœºåˆ¶è§£å†³æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œæ ¸å¿ƒå…¬å¼å¦‚ä¸‹ï¼š

#### è¾“å…¥é—¨ (Input Gate)
$$i_t = \sigma(W_{ii}x_t + W_{hi}h_{t-1} + b_i)$$

#### é—å¿˜é—¨ (Forget Gate)
$$f_t = \sigma(W_{if}x_t + W_{hf}h_{t-1} + b_f)$$

#### å€™é€‰è®°å¿† (Candidate Memory)
$$\tilde{c}_t = \tanh(W_{ig}x_t + W_{hg}h_{t-1} + b_g)$$

#### è¾“å‡ºé—¨ (Output Gate)
$$o_t = \sigma(W_{io}x_t + W_{ho}h_{t-1} + b_o)$$

#### ç»†èƒçŠ¶æ€æ›´æ–°
$$c_t = f_t \odot c_{t-1} + i_t \odot \tilde{c}_t$$

#### éšè—çŠ¶æ€æ›´æ–°
$$h_t = o_t \odot \tanh(c_t)$$

å…¶ä¸­ï¼š
- $\sigma$ è¡¨ç¤ºSigmoidå‡½æ•°
- $\odot$ è¡¨ç¤ºå…ƒç´ çº§ä¹˜æ³•
- $W_*$ è¡¨ç¤ºæƒé‡çŸ©é˜µï¼Œ$b_*$ è¡¨ç¤ºåç½®å‘é‡

### 2. RNNï¼ˆå¾ªç¯ç¥ç»ç½‘ç»œï¼‰

RNNçš„æ ¸å¿ƒå…¬å¼ç›¸å¯¹ç®€å•ï¼š

$$h_t = \tanh(W_{ih}x_t + W_{hh}h_{t-1} + b_h)$$

è¾“å‡ºï¼š
$$y_t = W_{ho}h_t + b_o$$

### 3. MLPï¼ˆå¤šå±‚æ„ŸçŸ¥æœºï¼‰

å‰å‘ä¼ æ’­ï¼š
$$h^{(l)} = \sigma(W^{(l)}h^{(l-1)} + b^{(l)})$$

å…¶ä¸­ $l$ è¡¨ç¤ºå±‚æ•°ï¼Œ$\sigma$ ä¸ºæ¿€æ´»å‡½æ•°ã€‚

### 4. æ³¨æ„åŠ›æœºåˆ¶

æœ¬é¡¹ç›®å®ç°çš„æ³¨æ„åŠ›æœºåˆ¶åŸºäº**åŠ æ€§æ³¨æ„åŠ›**ï¼š

#### æ³¨æ„åŠ›åˆ†æ•°è®¡ç®—
$$e_t = v_a^T \tanh(W_a h_t)$$

å…¶ä¸­ $h_t$ æ˜¯ç¬¬ $t$ ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€ã€‚

#### Softmaxå½’ä¸€åŒ–
$$\alpha_t = \frac{\exp(e_t)}{\sum_{k=1}^{T} \exp(e_k)}$$

#### ä¸Šä¸‹æ–‡å‘é‡
$$c = \sum_{t=1}^{T} \alpha_t h_t$$

æœ€ç»ˆé¢„æµ‹ï¼š
$$y = \text{Linear}(c)$$

## ğŸ“Š ä¼˜åŒ–ç®—æ³•åŸç†

### 1. SGDï¼ˆéšæœºæ¢¯åº¦ä¸‹é™ï¼‰

å‚æ•°æ›´æ–°å…¬å¼ï¼š
$$\theta_{t+1} = \theta_t - \eta \nabla_\theta L(\theta_t)$$

å…¶ä¸­ï¼š
- $\theta$ è¡¨ç¤ºå‚æ•°
- $\eta$ è¡¨ç¤ºå­¦ä¹ ç‡
- $\nabla_\theta L$ è¡¨ç¤ºæŸå¤±å‡½æ•°å¯¹å‚æ•°çš„æ¢¯åº¦

### 2. Adamä¼˜åŒ–å™¨

Adamç»“åˆäº†åŠ¨é‡å’Œè‡ªé€‚åº”å­¦ä¹ ç‡ï¼š

#### ä¸€é˜¶çŸ©ä¼°è®¡ï¼ˆåŠ¨é‡ï¼‰
$$v_t = \beta_1 v_{t-1} + (1-\beta_1)g_t$$

#### äºŒé˜¶çŸ©ä¼°è®¡ï¼ˆæ¢¯åº¦å¹³æ–¹ï¼‰
$$s_t = \beta_2 s_{t-1} + (1-\beta_2)g_t^2$$

#### åå·®æ ¡æ­£
$$\hat{v}_t = \frac{v_t}{1-\beta_1^t}$$
$$\hat{s}_t = \frac{s_t}{1-\beta_2^t}$$

#### å‚æ•°æ›´æ–°
$$\theta_{t+1} = \theta_t - \eta \frac{\hat{v}_t}{\sqrt{\hat{s}_t} + \epsilon}$$

é»˜è®¤è¶…å‚æ•°ï¼š
- $\beta_1 = 0.9$ï¼ˆä¸€é˜¶çŸ©è¡°å‡ç‡ï¼‰
- $\beta_2 = 0.999$ï¼ˆäºŒé˜¶çŸ©è¡°å‡ç‡ï¼‰
- $\epsilon = 10^{-8}$ï¼ˆæ•°å€¼ç¨³å®šæ€§ï¼‰

## ğŸ“ˆ æŸå¤±å‡½æ•°

### MSEï¼ˆå‡æ–¹è¯¯å·®ï¼‰

$$L = \frac{1}{N}\sum_{i=1}^{N}(y_i - \hat{y}_i)^2$$

æ¢¯åº¦ï¼š
$$\frac{\partial L}{\partial \hat{y}_i} = \frac{2}{N}(\hat{y}_i - y_i)$$

## ğŸ”§ æ ¸å¿ƒæ¨¡å—è¯¦è§£

### 1. MatrixSlowæ¡†æ¶æ ¸å¿ƒ (core_framework.py)

```python
class Node:
    def forward(self):
        """å‰å‘ä¼ æ’­ï¼šé€’å½’è®¡ç®—çˆ¶èŠ‚ç‚¹å€¼ï¼Œç„¶åè®¡ç®—å½“å‰èŠ‚ç‚¹"""
        for parent in self.parents:
            if parent.value is None:
                parent.forward()
        self.compute()
    
    def backward(self, result):
        """åå‘ä¼ æ’­ï¼šä½¿ç”¨é“¾å¼æ³•åˆ™è®¡ç®—é›…å¯æ¯”çŸ©é˜µ"""
        # å®ç°è§ä¸Šæ–‡è‡ªåŠ¨å¾®åˆ†åŸç†
```

### 2. å‚æ•°åˆå§‹åŒ–ç­–ç•¥

#### Heåˆå§‹åŒ–ï¼ˆç”¨äºReLUï¼‰
$$W \sim \mathcal{N}(0, \sqrt{\frac{2}{n_{in}}})$$

#### Xavieråˆå§‹åŒ–ï¼ˆç”¨äºSigmoid/Tanhï¼‰
$$W \sim \mathcal{U}(-\sqrt{\frac{6}{n_{in} + n_{out}}}, \sqrt{\frac{6}{n_{in} + n_{out}}})$$

### 3. æ•°å€¼ç¨³å®šæ€§å¤„ç†

1. **æ¢¯åº¦è£å‰ª**ï¼šé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
   ```python
   if gradient_norm > max_norm:
       gradient = gradient * max_norm / gradient_norm
   ```

2. **Sigmoidæ•°å€¼ç¨³å®š**ï¼š
   ```python
   x = np.clip(x, -500, 500)  # é˜²æ­¢æ•°å€¼æº¢å‡º
   ```

3. **é—å¿˜é—¨åç½®åˆå§‹åŒ–**ï¼š
   ```python
   self.b_f = Parameter(np.ones((1, hidden_size)))  # åˆå§‹åŒ–ä¸º1
   ```

## ğŸ“Š å®éªŒè®¾è®¡åŸç†

### 1. æ—¶é—´åºåˆ—é¢„æµ‹

ä½¿ç”¨**æ»‘åŠ¨çª—å£**æ–¹æ³•ï¼š
- è¾“å…¥ï¼šè¿‡å»30å¤©çš„å¼€ç›˜ä»· $[x_{t-29}, x_{t-28}, ..., x_t]$
- è¾“å‡ºï¼šç¬¬31å¤©çš„å¼€ç›˜ä»· $x_{t+1}$

### 2. æ•°æ®é¢„å¤„ç†

#### æ ‡å‡†åŒ–
$$x_{normalized} = \frac{x - \mu}{\sigma}$$

å…¶ä¸­ $\mu$ ä¸ºå‡å€¼ï¼Œ$\sigma$ ä¸ºæ ‡å‡†å·®ã€‚

### 3. è¯„ä¼°æŒ‡æ ‡

#### MAEï¼ˆå¹³å‡ç»å¯¹è¯¯å·®ï¼‰
$$\text{MAE} = \frac{1}{n}\sum_{i=1}^{n}|y_i - \hat{y}_i|$$

#### æ³›åŒ–æ¯”ç‡
$$\text{Generalization Ratio} = \frac{\text{Test MAE}}{\text{Train MAE}}$$

## ğŸ’¡ å…³é”®æŠ€æœ¯å®ç°ç»†èŠ‚

### 1. åŠ¨æ€è®¡ç®—å›¾ç®¡ç†

```python
# æ¯æ¬¡å‰å‘ä¼ æ’­å‰æ¸…ç†è®¡ç®—å›¾
default_graph.clear_jacobi()
default_graph.reset_value()
```

### 2. æ‰¹å¤„ç†å®ç°

```python
def create_batch(X, y, batch_size):
    for start in range(0, len(X), batch_size):
        end = min(start + batch_size, len(X))
        yield X[start:end], y[start:end]
```

### 3. åºåˆ—å¤„ç†

LSTM/RNNçš„åºåˆ—è¾“å…¥å¤„ç†ï¼š
```python
x_sequence = []
for t in range(seq_len):
    x_t = Variable((batch_size, input_size))
    x_t.set_value(X_batch[:, t].reshape(batch_size, input_size))
    x_sequence.append(x_t)
```

## ğŸ¯ æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

1. **å†…å­˜ä¼˜åŒ–**ï¼š
   - åŠæ—¶æ¸…ç†è®¡ç®—å›¾
   - ä½¿ç”¨ç”Ÿæˆå™¨å¤„ç†æ‰¹æ¬¡æ•°æ®

2. **è®­ç»ƒç¨³å®šæ€§**ï¼š
   - ä½¿ç”¨Adamä¼˜åŒ–å™¨
   - åˆç†çš„å­¦ä¹ ç‡è¡°å‡
   - æ—©åœæœºåˆ¶

3. **æ•°å€¼ç¨³å®šæ€§**ï¼š
   - æ¿€æ´»å‡½æ•°å€¼åŸŸé™åˆ¶
   - æ¢¯åº¦è£å‰ª
   - åˆç†çš„å‚æ•°åˆå§‹åŒ–

## ğŸ“ ä»£ç ç¤ºä¾‹

### åˆ›å»ºå’Œè®­ç»ƒLSTMæ¨¡å‹

```python
# åˆ›å»ºLSTMæ¨¡å‹
lstm = LSTM(input_size=1, hidden_size=32, num_layers=1)

# è®­ç»ƒæ¨¡å‹
train_losses, val_losses, output_layer = train_model(
    lstm, 'lstm', 
    (X_train, y_train), 
    (X_test, y_test),
    epochs=20, 
    batch_size=16, 
    learning_rate=0.01
)

# é¢„æµ‹
predictions = predict_with_model(lstm, 'lstm', X_test, output_layer)
```

## ğŸ› å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

1. **æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸**
   - è§£å†³ï¼šä½¿ç”¨æ¢¯åº¦è£å‰ªï¼Œåˆç†åˆå§‹åŒ–å‚æ•°
   - LSTMé—å¿˜é—¨åç½®åˆå§‹åŒ–ä¸º1

2. **å†…å­˜ä¸è¶³**
   - è§£å†³ï¼šå‡å°batch_sizeï¼ŒåŠæ—¶æ¸…ç†è®¡ç®—å›¾

3. **è®­ç»ƒä¸æ”¶æ•›**
   - è§£å†³ï¼šé™ä½å­¦ä¹ ç‡ï¼Œä½¿ç”¨Adamä¼˜åŒ–å™¨

## ğŸ“Š å®éªŒç»“æœåˆ†æ

å…¸å‹æ€§èƒ½å¯¹æ¯”ï¼ˆä»…ä¾›å‚è€ƒï¼‰ï¼š

| æ¨¡å‹ | å‚æ•°é‡ | è®­ç»ƒæ—¶é—´ | æµ‹è¯•MAE | è¯´æ˜ |
|------|--------|----------|---------|------|
| LSTM_32 | ~5K | ä¸­ç­‰ | 0.XXXX | åŸºç¡€é…ç½® |
| LSTM_64 | ~20K | è¾ƒé•¿ | 0.XXXX | å¢å¤§å®¹é‡ |
| LSTM+Attention | ~10K | é•¿ | 0.XXXX | æ³¨æ„åŠ›å¢å¼º |
| MLP | ~3K | å¿« | 0.XXXX | æ— æ—¶åºå»ºæ¨¡ |
| RNN_32 | ~2K | ä¸­ç­‰ | 0.XXXX | ç®€å•æ—¶åº |

## ğŸ‘¨â€ğŸ’» ä½œè€…ä¿¡æ¯

- è¯¾ç¨‹ï¼šåŸºäºPythonçš„æ·±åº¦å­¦ä¹ æ¡†æ¶å®ç°
- å­¦æ ¡ï¼šæ·±åœ³å¤§å­¦
- æ—¶é—´ï¼š2025å¹´6æœˆ

## ğŸ“„ License

æœ¬é¡¹ç›®ä»…ç”¨äºæ•™å­¦ç›®çš„ï¼Œè¯·å‹¿ç”¨äºå•†ä¸šç”¨é€”ã€‚

---

**æ³¨æ„**ï¼š
1. è¿è¡Œå‰è¯·ç¡®ä¿ `Daily_ZX.csv` æ–‡ä»¶åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹
2. æœ¬é¡¹ç›®å®Œå…¨åŸºäºNumPyå®ç°ï¼Œæ— éœ€å®‰è£…PyTorch/TensorFlow
3. é¦–æ¬¡è¿è¡Œå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´è¿›è¡Œæ¨¡å‹è®­ç»ƒ
